{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (PCA)- A tool for dimensionality reduction\n",
    "\n",
    "## The purpose of PCA and some intuition\n",
    "\n",
    "*If you're familiar with how PCA works feel free to skip straight to the coding part - if not, read on!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this era of Big Data, it can be possible to have *too much* data. We might have so much data that it makes our brains or computers crash when we try to process it. What can we do when this happens?\n",
    "\n",
    "The simplest answer is to just throw some of the data away. If we have a dataset $X$ with $n$ examples and $p$ features, we could get rid of some of the examples to have a more manageable dataset of size $m \\times p$ (where $m < n$) or we could randomly pick some features and throw them away to yield a dataset of size $n \\times q$.\n",
    "\n",
    "This approach, whilst quick and easy, is far from optimal. Just throwing information away usually makes our models worse and modelling complex relationships between features is hard enough already!\n",
    "\n",
    "We still want to reduce the size of our dataset though, so consider the following approach: We sift through each feature in the dataset and look for ones we can get rid of. How do we decide if a feature is a suitable candidate to be discarded? Let's think of an example: Suppose we were trying to use information about houses (size in $m^2$, distance to the nearest shop, number of bedrooms etc.) to predict house prices and we noticed that all of the houses in our dataset were 3 bedroom houses. The *number of bedrooms* feature would be an excellent feature to be discarded because there is no variability for that feature in the dataset - it doesn't allow us to discriminate between different houses and so is of no use to us. \n",
    "\n",
    "Although it's an improvement on the random deletion approach, sifting through all of the features suffers from two main drawbacks:\n",
    "\n",
    "1. It would be rare that a feature conveys absolutely no information at all (as is the case in the housing example above) and we'd like to hang on to as much information as we can.\n",
    "2. The approach of looking at every variable in a high-dimensional dataset (which might have hundreds of features) is something no one should be forced to do unless there's a very good reason to! We need an approach that can easily be automated and scales nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Finding the directions of maximal variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components analysis operates using a similar principle to the one we described above. It works as follows for a $n \\times p$ dataset $X$.\n",
    "\n",
    "1. Find the direction in p-dimensional space where the data exhibits the most variation (Note: this direction is not limited to the p axes, it can be literally any direction at all). This is called the Principal Component.\n",
    "2. Find the direction in p-dimensional space which is *orthogonal* to the Principal Component which exhibits the most variation. This direction is called the Second Principal Component\n",
    "3. Find the direction in p-dimensional space which is *orthogonal* to *both the Principal Component and the Second Principal Component* which exhibits the most variation. This direction is called the Third Principal Component.\n",
    "4. Continue this process until you have p Principal Components - each one orthogonal to all the others.\n",
    "\n",
    "These $p$ Principal Components define a new coordinate system for the data - the key point to recognise is that the data are still the same, they're just being *represented* differently. If you've not thought about changing coordinate systems before, it might help to think of a real world example where we use alternative coordinate systems:\n",
    "\n",
    "### An example of changing coordinates - Different representations of colour: RGB vs CMYK\n",
    "\n",
    "One way of precisely defining the shade of a colour is to express it using the RGB system - although we're skipping over the finer details here, we express a colour as a vector $(x_1, x_2, x_3)$ where $x_1, x_2, x_3$ represent the intensities of red, green and blue, respectively. For example:\n",
    "\n",
    "* (255, 0, 0) represents pure red\n",
    "* (0,255,0) represents pure green\n",
    "* (255, 255, 0) represents pure yellow\n",
    "\n",
    "Instead of using red, green and blue as the basis for describing a colour, some processes (most notably your printer) use an alternative basis. They use the respective intensities of cyan, magenta and yellow to describe a colour. Under this system, we would represent pure yellow as (0, 0, 255) - it's exactly the same colour that we would get representing it as (255, 255, 0) under the RGB system, *but it just has a different representation under a different coordinate system*. \n",
    "\n",
    "### Back to the Principal Components\n",
    "\n",
    "Changing coordinate systems in this way gives us a new dataset $X^*$ (still with dimensions $n \\times p$). Going back to the house prices example above, our features no longer have convenient names such as *size in $m^2$*, but they do have the benefit that we know which ones have the most variance (and are therefore theoretically most likely to be useful in modelling the label, which is our goal). If we want to work with a smaller dataset, we can now just discard the $(k+1)^{th} \\text{ to } p^{th}$ Principal Components and fit a model using the first $k$ Principal Components. In theory these $k$ features will contain more information than any $k$ features in the original dataset and should give us a better model for the same computational expense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation \n",
    "\n",
    "Before we describe how we actually go about finding the Principal Components, let's generate the dataset we're going to transform and visualise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200 #Number of observations\n",
    "\n",
    "x1 = np.random.uniform(0,10,n)\n",
    "x2 = x1 + np.random.normal(0,2,n)\n",
    "\n",
    "X = pd.DataFrame({'X1':x1, 'X2':x2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x1, x2)\n",
    "plt.xlim(-5,15)\n",
    "plt.ylim(-5,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset only has two variables, so ordinarily we wouldn't bother reducing its dimension because it's manageable enough already. However for the purposes of illustration it's easy for us to visualise the change when we turn a 2D dataset into a 1D dataset.\n",
    "\n",
    "Now it might not have been clear above exactly how we would go about finding out which direction in the 2D plane has the most variation. To do so, we have to *project* the data onto that direction. Lets say we have a point $(x_1, x_2)$ and the direction we want to project it onto is the line $l$ with equation $X_2 = m \\times X_1 + c$. To project $(x_1, x_2)$ onto the line $l$ we simply draw a line which is perpendicular to $l$ and mark the point where they intersect. That intersection point is then the projection of $(x_1, x_2)$ onto $l$. \n",
    "\n",
    "Below is a few of examples of projecting our dataset $X$ onto different lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = np.array([0,1]) #The line x = 0\n",
    "line2 = np.array([np.sqrt(2), np.sqrt(2)]) #The line y = x\n",
    "line3 = np.array([-np.sqrt(2),np.sqrt(2)]) #The line y = -x\n",
    "\n",
    "def projectPointOntoLine(point, line):\n",
    "    \n",
    "    return (np.dot(point, line)/np.dot(line, line))*line\n",
    "\n",
    "projLine1 = np.array([projectPointOntoLine(x, line1) for idx, x in X.iterrows()]) #Projection onto line 1\n",
    "projLine2 = np.array([projectPointOntoLine(x, line2) for idx, x in X.iterrows()]) #Projection onto line 2\n",
    "projLine3 = np.array([projectPointOntoLine(x, line3) for idx, x in X.iterrows()]) #Projection onto line 3\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,6))\n",
    "fig.suptitle('Projecting X onto different lines')\n",
    "ax1.scatter(projLine1[:,0], projLine1[:,1])\n",
    "ax1.set_xlim((-5,15)); ax1.set_ylim((-5,15))\n",
    "ax1.title.set_text('The line x = 0')\n",
    "\n",
    "ax2.scatter(projLine2[:,0], projLine2[:,1])\n",
    "ax2.set_xlim((-5,15)); ax2.set_ylim((-5,15))\n",
    "ax2.title.set_text('The line y = x')\n",
    "\n",
    "ax3.scatter(projLine3[:,0], projLine3[:,1])\n",
    "ax3.set_xlim((-5,15)); ax3.set_ylim((-5,15))\n",
    "ax3.title.set_text('The line y = -x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately we can see that the points are more spread out when we project onto the line $y = x$ than if we project onto the line $y = -x$. This gives us a good indication that there is more variability in the direction given by the line $y = x$ than in the direction given by the line $y = -x$.\n",
    "\n",
    "This is of course a very ad-hoc and non-rigourous method for determining the direction in which the data exhibit the most variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Principal Components\n",
    "\n",
    "We won't go into a full derivation of how to find the Principal Components - there are lots of resources on the internet where you can find such a proof.\n",
    "\n",
    "To obtain the Principal Components for a dataset, $X$ of dimension $n \\times p$:\n",
    "\n",
    "1. Compute the covariance matrix of $X$. The $p \\times p$ covariance matrix is computed as $S = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar x)(x_i - \\bar x) = \\frac{1}{n-1}X^TX$, where $x_i$ is a single observation of dimension $p$.\n",
    "2. Compute all $p$ eigenvalues $(\\lambda_1, ..., \\lambda_p)$ for $S$ and their associated eigenvectors $(v_1,...,v_p)$, such that $Sv_i = \\lambda_iv_i$\n",
    "3. Each eigenvector represents a direction, and the Principal Component is the eigenvector corresponding to the largest eigenvalue. The 2nd Principal Component is the eigenvector corresponding to the second largest eigenvalue and so on.\n",
    "4. To compute a reduced-dimension dataset of dimension $n \\times k$, $X^*$, compute $X^* = XV_{1:k}$, where $V_{1:k} = [v^*_1, v^*_2,...,v^*_k]$, the matrix of the first k Principal Components concatenated together (of dimension $p \\times k$. This step projects each of the examples onto each of the first $k$ Principal Components\n",
    "\n",
    "\n",
    "\n",
    "## *If you just wanted to get on with implementing the algorithm, here is the part to start paying attention again!*\n",
    "\n",
    "\n",
    "\n",
    "# Implementing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.dataArray = data.to_numpy() #Occasionally we'll want to multiply arrays together \n",
    "\n",
    "        \n",
    "    def computeCovMatrix(self):\n",
    "        pass\n",
    "    \n",
    "    def getEigenvalueDecomposition(self):\n",
    "        pass\n",
    "    \n",
    "    def fitPCA(self, numComponents):\n",
    "        #Return the first numComponents principal components\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPCA = PCA(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XStar = myPCA.fitPCA(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XStar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,10))\n",
    "ax1.scatter(XStar['PC1'], XStar['PC2'])\n",
    "ax1.set_xlim((-20,5))\n",
    "ax1.set_ylim([-20, 5])\n",
    "ax1.set(xlabel='Principal Component', ylabel='Second Principal Component', title = 'Principal Components')\n",
    "\n",
    "#Also want to plot the original data with the first and second pc running through\n",
    "\n",
    "#First PC\n",
    "principalComponent = myPCA.Vk[:,0]\n",
    "pcDirection = principalComponent[1]/principalComponent[0]\n",
    "\n",
    "#Second PC\n",
    "principalComponent2 = myPCA.Vk[:,1]\n",
    "pcDirection2 = principalComponent2[1]/principalComponent2[0]\n",
    "\n",
    "\n",
    "ax2.scatter(X['X1'], X['X2'])\n",
    "ax2.set_xlim((-5,15))\n",
    "ax2.set_ylim([-5,15])\n",
    "ax2.set(xlabel='X1', ylabel='X2', title = 'Original Data')\n",
    "#Plot their lines\n",
    "ax2.plot(np.linspace(-5,15), pcDirection*np.linspace(-5,15), c = 'g',linewidth = 6, label = 'Principal Component')\n",
    "ax2.plot(np.linspace(-5,15), 10 + pcDirection2*np.linspace(-5,15), c = 'r', linewidth = 6, label = '2nd Principal Component')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the x axis of the left plot, which corresponds to the first Principal Component, exhibits a lot more variation than the corresponding y axis\n",
    "\n",
    "On the right hand plot we've plotted the original dataset with the directions corresponding to the Principal Components running through the data, the direction with the greatest variance looks like we might have intuitively expected it to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cgvae)",
   "language": "python",
   "name": "cgvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
