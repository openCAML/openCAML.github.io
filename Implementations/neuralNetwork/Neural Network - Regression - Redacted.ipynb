{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "The main purpose of this notebook is to help you understand how the process of backpropagation helps us to train a neural network by tuning the weights to maximise predictive accuracy. Readers should be familiar with the general concept of neural networks before attempting to fill in the notebook. \n",
    "\n",
    "For a more formal explanation of backpropagation, Bishop's [*Pattern Recognition and Machine Learning*](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) covers it in detail in section 5.3. I found it to be useful to sit down with a pen and paper to draw the network diagrams and map how the inputs and error move forwards and backwards through the network respectively!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "Use the generative process of a linear model - i.e. a weighted sum of the features plus Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000 #Number of observations in the training set\n",
    "p = 5 #Number of parameters, including intercept\n",
    "\n",
    "beta = np.random.uniform(-10, 10, p) #Randomly initialise true parameters\n",
    "\n",
    "for i, b in enumerate(beta):\n",
    "    print(f'\\u03B2{i}: {round(b, 3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0,10,(n,(p-1))) #Randomly sample features X1-X4\n",
    "X0 = np.array([1]*n).reshape((n,1)) #X0 is our intercept so always equal to 1\n",
    "X = np.concatenate([X0,X], axis = 1) #Join intercept to other variables to form feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.matmul(X,beta) + np.random.normal(0,10,n) #Linear combination of the features plus a normal error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate to create dataframe\n",
    "dataFeatures = pd.DataFrame(X)\n",
    "dataFeatures.columns = [f'X{i}' for i in range(p)] #Name feature columns\n",
    "\n",
    "dataTarget = pd.DataFrame(Y)\n",
    "dataTarget.columns = ['Y'] #Name target \n",
    "\n",
    "data = pd.concat([dataFeatures, dataTarget], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly visualise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of Rows: {data.shape[0]}')\n",
    "print(f'Number of Columns: {data.shape[1]}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a neural network\n",
    "\n",
    "We'll use a single hidden layer and tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, data, target, features, hiddenSize, trainTestRatio = 0.9):\n",
    "        \n",
    "        self.target = target\n",
    "        self.features = features \n",
    "        \n",
    "        #Split up data into a training and testing set\n",
    "        self.train, self.test = train_test_split(data, test_size=1-trainTestRatio)\n",
    "        \n",
    "        self.input      = np.array(self.train[self.features])\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.weightsInputToHidden   = np.random.normal(size = (self.input.shape[1],hiddenSize))\n",
    "        self.weightsHiddenToOutput   = np.random.normal(size = (hiddenSize + 1 ,))   #+1 is for the bias term              \n",
    "        self.y          = np.array(self.train[self.target])\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "        \n",
    "        \n",
    "        #Standardise training set\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(self.input)\n",
    "        self.input = self.scaler.transform(self.input)\n",
    "        \n",
    "        #Pre-allocate weight derivatives\n",
    "        self.dWeightsInputToHidden = np.ones(self.weightsInputToHidden.shape)\n",
    "        self.dWeightsHiddenToOutput = np.ones(self.weightsHiddenToOutput.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        \n",
    "        #Compute hidden activations, a, and transform them with tanh activation\n",
    "        self.a = #...\n",
    "        self.z = #...\n",
    "        \n",
    "        #Add bias term onto z for the next layer of the network\n",
    "        \n",
    "        #Code goes here...\n",
    "        \n",
    "        \n",
    "        self.z = self.zWithBias\n",
    "        \n",
    "        #Compute Output\n",
    "        \n",
    "\n",
    "        \n",
    "    def backpropagation(self):\n",
    "        \n",
    "        normFactor = 1/self.input.shape[0] #Normalising factor for the derivatives\n",
    "        \n",
    "        #Compute Deltas\n",
    "        #self.deltaOutput and self.deltaHidden\n",
    "    \n",
    "        #Compute Weight derivatives:\n",
    "        \n",
    "        \n",
    "        self.dWeightsInputToHidden = #...Make sure dimensions match up with weight matrix\n",
    "        self.dWeightsHiddenToOutput =#...\n",
    "\n",
    "    def trainNetwork(self, lr = 0.001, numEpochs = 100):\n",
    "        \n",
    "        #Train by feeding the data through the network and then backpropagating error a set number (numEpochs) of times\n",
    "        #Apply gradient descent to update the weights\n",
    "        #Stop training early if the gradients vanish\n",
    "        \n",
    "        ep = 0\n",
    "        \n",
    "        \n",
    "        while ep < numEpochs and (np.linalg.norm(self.dWeightsInputToHidden) + np.linalg.norm(self.dWeightsHiddenToOutput)) > 0.5:\n",
    "            \n",
    "            #feedforward and backpropagate\n",
    "            \n",
    "            #Update weights\n",
    "            \n",
    "            #update ep\n",
    "        print('Training completed')\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \n",
    "        #Works in the same way as feedforward:\n",
    "        \n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInput = np.array(data[['X0', 'X1', 'X2', 'X3', 'X4']])\n",
    "dataOutput = np.array(data['Y'])\n",
    "myNN = NeuralNetwork(data, 'Y', ['X0', 'X1', 'X2', 'X3', 'X4'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNN.feedforward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNN.trainNetwork(lr= 0.001, numEpochs=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how our model performs\n",
    "\n",
    "Lets predict the labels of the held out test set and plot them against the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTest = myNN.predict(myNN.test[myNN.features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the poins gather around the line y = x then our model is performing as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(myNN.test[myNN.target], predTest)\n",
    "plt.plot(np.arange(-100,100), np.arange(-100,100))\n",
    "plt.xlabel('True Label')\n",
    "plt.ylabel('Predicted Label (Neural Network)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In reality, the dataset we used to fit our neural network was not a particularly challenging one due to the relationships between variables being linear - you can check to see that using a linear regression on this dataset would give comparable predictive accuracy.\n",
    "\n",
    "The value in neural networks, though, lies in their ability to approximate more complex relationships by adding more hidden layers to the model architecture. We will see in the notebooks on 'deeper' neural networks (to be published soon, hopefully!) that such models outperform simpler models in terms of predictive accuracy - now that we've got the hang of backpropagation in the limited case presented in this notebook, it's not a big mental step to extend it to a neural network with any number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cgvae)",
   "language": "python",
   "name": "cgvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
