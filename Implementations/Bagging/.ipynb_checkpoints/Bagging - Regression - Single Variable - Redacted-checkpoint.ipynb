{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging - Bootstrapped Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we implemented the single feature decision tree, we saw that essentially we were fitting a step function. We obtained a split point, and if the feature was smaller than the split point, we assigned one value and if it was bigger then we assigned a different value. We enabled more flexibility by implementing a decision tree that could handle multiple features and allowed us to specify a depth to learn more non-linear relationships\n",
    "\n",
    "A slight issue with decision trees is that they can be sensitive to slight changes in the data. Depending on the precise locations of the split points, adding a new point could slightly change the location of the split points and send observations to completely different terminal nodes which might assign very different values. \n",
    "\n",
    "In this notebook we're going to implement a method called Bagging (Bootstrapped Aggregation), which can help us learn more smooth regression lines, so that we're less susceptible to jumps in predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data for Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a single feature variable so that we can easily visualise the model fitting process, but to avoid making it too easy for our decision trees we're going to make the relationship between feature and target horribly non-linear. We'll use the generating equation:\n",
    "\n",
    "$$y = x \\times (\\text{sin}^2(x) - \\text{cos}(x^2)) + N(0, \\sigma^2)$$\n",
    "\n",
    "Where $N(0, \\sigma^2)$ is a sample from a zero-mean normal distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "x = np.random.uniform(0, 4, n)\n",
    "\n",
    "def generateY(x, sd):\n",
    "    return x*(np.sin(x)**2 - np.cos(x**2)) + np.random.normal(0,sd,1)\n",
    "\n",
    "y = [generateY(i,  0.5)[0] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'X':x, 'Y':y}) #Put the two arrays into a dataframe to make it easy to work with\n",
    "data.head(10) #Inspect the first ten rows of our dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickly plot the data so we know what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['X'], data['Y'], s = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully we can see that fitting a single split point decision tree here will yield poor results. The data isn't well set up to be approximated by a step function, so a smoother function will be needed to achieve good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does Bagging work?\n",
    "\n",
    "Bagging is the simplest member in the family of ensemble methods. **If you're well versed in ensemble methods then you can skip reading this part and head to the coding part**. \n",
    "\n",
    "The general idea behind an ensemble method is that crowds are more often correct than individuals. To give an illustration as to why this statement is (sometimes) true, think of the popular gameshow *Who Wants To Be A Millionaire?*. The game challenges contestants to correctly answer 15 multiple choice questions, where each question has four answers for the contestant to select from. If a contestant gets stuck on a particular question, they can elect to Ask The Audience. Each member of the audience then submits their answer via an electronic device and the contestant is shown the proportions with which the audience chose the relative answers and then answers the question taking this information into account.\n",
    "\n",
    "Why should the contestant listen to the audience? Let's say 1/3 of the audience know the correct answer with total certainty and the remaining 2/3 of the audience guess at random between the four answers. Under this framework, $\\frac{1}{3} + \\frac{1}{4} \\times \\frac{2}{3} = \\frac{1}{2}$ of the audience will report the correct answer and each of the incorrect answers will be reported 16.6% of the time. Therefore, under this very simplified model, and assuming that the contestant is not paranoid that the audience is out to get them, they should choose the answer most favoured by the audience.\n",
    "\n",
    "The WWTBAM? example is slightly different than the regression example that we're working with, and is of course totally devoid of rigour. A slightly more rigourous explanation is that the error in a model has two sources - bias and variance. A model which returns the average value of an ensemble of models will suffer from the same biases as the models in the ensemble, but by taking an average we reduce the variance component of the error (as the models which over-predict and those which under-predict should cancel each other out), leading to a model which in theory should have a smaller average error. To read more about the bias-variance decomposition and why ensemble methods work, I'd recommend section 14 of Bishop's PRML (available here: http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're going to adopt an ensemble-based approach, a very good question is: Where are we going to get our data from? If we had ten models in our ensemble and assigned a tenth of the training set to each model then the performance of each model might be drastically worse due to a lack of data and it might not be possible to get any more data. On the other hand if we use the same dataset to train each model, then each model will predict exactly the same thing, essentially rendering our ensemble approach pointless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping allows us to have the best of both worlds. If you've not heard of it before, the Bootstrap is an old-fashioned statistical trick to get more bang for your buck from your data. In simple terms, if we have a dataset \n",
    "\n",
    "$$X = \\{X_1, X_2, ..., X_n\\}$$\n",
    "\n",
    "then we can create a new dataset $X_{B_1}$ by taking samples from $X$. If we take $n$ samples from $X$ (with replacement), then $X_{B_1}$ is a different dataset but with a similar distribution of values to $X$. The process is supposed to imitate sampling from the true distribution which generated $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of obtaining $X_{B_1}$, a dataset which is similar to $X$ but not quite the same is that we can fit a decision tree using $X_{B_1}$ and it will give slightly different results than if we had used $X$. We can bootstrap lots of times to obtain $X_{B_2}, X_{B_3}, ...,X_{B_b}$, each one a different dataset which will in turn give rise to a different decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Bagged model, we simply aggregate the output of an ensemble of Bootstrapped models (hence the name: Bagging - Bootstrap AGGregatING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decisionTreeNode:\n",
    "    \n",
    "    def __init__(self,data, target, features, currentDepth): \n",
    "        self.left = None #Left child node\n",
    "        self.right = None #Left child node\n",
    "        self.currentDepth = currentDepth\n",
    "        self.data = data \n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        \n",
    "        \n",
    "        self.splitPointMesh = {}\n",
    "        \n",
    "        for feature in self.features:\n",
    "            #We have a set of features and to determine how we're going to split this dataset \n",
    "            #we'll go through each feature in turn and find an optimal split point for that feature\n",
    "            #Then we'll split using the feature which gives the smallest error for the dataset\n",
    "            #(This is not necessarily an optimal strategy but the combinatorial space is too big to \n",
    "            #investigate every possible tree)\n",
    "            \n",
    "            #So the first point of that is defining a mesh for each feature\n",
    "            \n",
    "#             meshMin = np.sort(self.data[feature])[1] \n",
    "#             meshMax = np.sort(self.data[feature])[-2]\n",
    "\n",
    "            meshMin = np.min(self.data[feature])\n",
    "            meshMax = np.max(self.data[feature])\n",
    "            self.splitPointMesh[feature] = np.linspace(meshMin, meshMax, 500)\n",
    "    \n",
    "    def computeMeansGivenSplitPoint(self, splitPoint, feature):\n",
    "        #Given a split point, we want to split the training set in two\n",
    "        #One containing all the points below the split point and one containing all the points above the split point\n",
    "        #The means are then the means of the targets in those datasets and they are the values we want to return\n",
    "        \n",
    "        belowSplitPoint = self.data.loc[self.data[feature] < splitPoint][self.target].mean()\n",
    "        aboveSplitPoint = self.data.loc[self.data[feature] >= splitPoint][self.target].mean()\n",
    "        \n",
    "        return belowSplitPoint, aboveSplitPoint\n",
    "    \n",
    "    def computeSquaredError(self, splitPoint, feature):\n",
    "        #Once we have a split point and a set of means, we need to have some way of identifying whether it's \n",
    "        #a good split point or not\n",
    "        \n",
    "        #First apply compuuteMeansGivenSplitPoint to get the values for above and below the dataset\n",
    "        #Then compute the sum of squared errors yielded by assigning the corresponding mean to each point in the training set\n",
    "        #If we add these two sums of squares together then we have a single error number which indicates how good our split point is\n",
    "        \n",
    "        c0, c1 = self.computeMeansGivenSplitPoint(splitPoint, feature)\n",
    "        \n",
    "        #To get the value of errorBelow, subset the training set to the points below the split points\n",
    "        #Then calculate the squared difference between target and c0 for each observation in the subset\n",
    "        #Then sum them up (This can all be done in one line)\n",
    "        \n",
    "        errorBelow = np.sum((self.data.loc[self.data[feature] < splitPoint][self.target] - c0)**2) \n",
    "        \n",
    "        #errorAbove works in the same way\n",
    "        errorAbove = np.sum((self.data.loc[self.data[feature] >= splitPoint][self.target] - c1)**2) \n",
    "        \n",
    "        totalError = errorBelow + errorAbove\n",
    "\n",
    "        return totalError\n",
    "    \n",
    "    def createSplitDatasetsGivenSplitPointAndFeature(self, splitPoint, feature):\n",
    "        #Given a split point, split the dataset up and return two datasets\n",
    "        \n",
    "        belowData = self.data.loc[self.data[feature] < splitPoint]\n",
    "        aboveData = self.data.loc[self.data[feature] >= splitPoint]\n",
    "        \n",
    "        return belowData, aboveData\n",
    "    \n",
    "\n",
    "def fitDT(Node, maxDepth):\n",
    "    \n",
    "    \n",
    "    if Node.currentDepth < maxDepth:\n",
    "        #if node depth > max depth then we continue to split\n",
    "        \n",
    "        #Do splitting here\n",
    "        #We want to find the best error for each of the features, then use that feature to do the splitting\n",
    "        \n",
    "        errors = {}\n",
    "        for feature in Node.features:\n",
    "            errors[feature] = [Node.computeSquaredError(splitPoint, feature) for splitPoint in Node.splitPointMesh[feature]]\n",
    "            \n",
    "        #Now we want to extract the feature and splitPoint which gave the best overall error\n",
    "        currentBestError = min(errors[Node.features[0]]) + 1 #Initialise\n",
    "        \n",
    "        for feature in Node.features:\n",
    "            if min(errors[feature]) < currentBestError:\n",
    "                bestFeature = feature\n",
    "                currentBestError = min(errors[feature])\n",
    "                bestSplitPoint = Node.splitPointMesh[feature][np.argmin(errors[feature])]\n",
    "                \n",
    "                \n",
    "        #Now we have the best feature to split on and the place where we should split it\n",
    "        splitDataLeft, splitDataRight = Node.createSplitDatasetsGivenSplitPointAndFeature(bestSplitPoint, bestFeature)\n",
    "        \n",
    "        #Record the splitting process\n",
    "        Node.featureSplitOn = bestFeature\n",
    "        Node.bestSplitPoint = bestSplitPoint\n",
    "        #print(bestFeature, bestSplitPoint)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if Node.data.drop_duplicates().shape[0] > 1:\n",
    "            Node.left = decisionTreeNode(splitDataLeft, Node.target, Node.features, Node.currentDepth + 1) #Define nodes on the levels below (increment depth by 1)\n",
    "            Node.right = decisionTreeNode(splitDataRight, Node.target, Node.features, Node.currentDepth + 1)\n",
    "            fitDT(Node.left, maxDepth) #The recursive part, which works exactly the same as we saw for the simpler example above\n",
    "            fitDT(Node.right, maxDepth)\n",
    "        else: #If there is only one example left in this dataset then there's no need to do any splitting\n",
    "            Node.left = copy.deepcopy(Node)\n",
    "            Node.right = copy.deepcopy(Node)\n",
    "            Node.left.currentDepth = Node.currentDepth + 1\n",
    "            Node.right.currentDepth = Node.currentDepth + 1\n",
    "            fitDT(Node.left, maxDepth) #The recursive part, which works exactly the same as we saw for the simpler example above\n",
    "            fitDT(Node.right, maxDepth)\n",
    "            \n",
    "        \n",
    "    elif Node.currentDepth == maxDepth:\n",
    "        #If we're at a terminal node then we need to return a value to predict\n",
    "        #Don't need to do any splitting or anything like that, just want to return the mean value\n",
    "        \n",
    "        Node.prediction = Node.data[Node.target].mean()\n",
    "            \n",
    "def predictSingleExample(decisionTreeNode, xrow, maxDepth):\n",
    "    #decisionTreeNode should be the root node of a fitted decision tree\n",
    "    #maxDepth needs to be the same maxDepth as the fitted decision tree\n",
    "    \n",
    "    #x needs to be a pandas dataframe (with one or more rows) with the same column names as the features \n",
    "    #in the training set\n",
    "    \n",
    "    if decisionTreeNode.currentDepth < maxDepth:\n",
    "\n",
    "        if xrow[decisionTreeNode.featureSplitOn] < decisionTreeNode.bestSplitPoint:\n",
    "            return predictSingleExample(decisionTreeNode.left, xrow, maxDepth)\n",
    "        else:\n",
    "            return predictSingleExample(decisionTreeNode.right, xrow, maxDepth)\n",
    "     \n",
    "\n",
    "    elif decisionTreeNode.currentDepth == maxDepth:\n",
    "        \n",
    "        return decisionTreeNode.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baggedDecisionTree:\n",
    "    \n",
    "    def __init__(self, data, target, features, trainTestRatio = 0.9, maxDepth = 5):\n",
    "        #data - a pandas dataset \n",
    "        #target - the name of the pandas column which contains the true labels\n",
    "        #feature - the name of the pandas column which we will use to do the regression\n",
    "        #trainTestRatio - the proportion of the entire dataset which we'll use for training\n",
    "                    #   - the rest will be used for testing\n",
    "        \n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        \n",
    "        #Max Depth of decision tree\n",
    "        self.maxDepth = maxDepth\n",
    "        \n",
    "        #Split up data into a training and testing set\n",
    "        self.train, self.test = train_test_split(data, test_size=1-trainTestRatio) #We're doing the train/test split here\n",
    "                                                                                 #Instead of in the DT\n",
    "            \n",
    "    \n",
    "    def fitBaggedDT(self, numTrees = 50):\n",
    "        \n",
    "        self.forest = [] #List containing all of our decision trees\n",
    "        \n",
    "        for i in range(numTrees):\n",
    "            \n",
    "            if i % 1 == 0:\n",
    "                print(f'Fitting decision Tree {i+1} of {numTrees}')\n",
    "            \n",
    "            #Bootstrap dataframe - sampling with replacement\n",
    "            \n",
    "            \n",
    "            #Define a decision tree and fit it\n",
    "            \n",
    "            #Save the fitted model\n",
    "            \n",
    "        return 0\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #X should be a pandas dataframe\n",
    "        #Predict the values for each tree in the forest and average them\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's fit a Bagged model!\n",
    "\n",
    "We'll use a forest of five trees, each with a depth of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = baggedDecisionTree(data, 'Y', ['X'], maxDepth = 5)\n",
    "bag.fitBaggedDT(numTrees = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.testPred = bag.predict(bag.test) #Predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to see how our bagged decision tree stacks up against an individual decsion tree\n",
    "\n",
    "There are far more rigourous ways to assess their relative performance but it can be quite instructive to visualise the regression lines of the bagged predictor compared to the trees that make up the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshMin = np.min(bag.train[bag.features])\n",
    "meshMax = np.max(bag.train[bag.features])\n",
    "bag.regLineMesh = np.linspace(meshMin, meshMax, 1000) #We're going to evaluate our models lots of times \n",
    "                                                    #then draw a line between the points\n",
    "\n",
    "bag.regLineDF = pd.DataFrame({'X':bag.regLineMesh.reshape(-1)}) #Put it into dataframe form so it can be fed to our predict function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.regLineDF['Pred'] = bag.predict(bag.regLineDF) #Predictions from the bagged predictor\n",
    "\n",
    "#Now do predictions for each tree in the forest\n",
    "\n",
    "for i in range(len(bag.forest)): #bag.forest contains each decision tree in the ensemble\n",
    "    bag.regLineDF[f'Pred{i}'] = [predictSingleExample(bag.forest[i], row, bag.maxDepth) for index, row in bag.regLineDF.iterrows()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10)) #Make figure big so we can see all of the lines\n",
    "plt.scatter(bag.train[bag.features], bag.train[bag.target], label = 'Training set')\n",
    "plt.plot(bag.regLineDF['X'], bag.regLineDF['Pred'], linestyle = 'solid', linewidth = 12,color = 'orange', label = 'Bagged Decision Trees')\n",
    "for i in range((len(bag.forest))):\n",
    "    plt.plot(bag.regLineDF['X'], bag.regLineDF[f'Pred{i}'], linestyle = 'solid', linewidth = 2,color = 'cyan', label = 'Individual Decision Tree')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the Cyan coloured lines corresponds to an individual decision tree. As we would expect, the Orange line is typically positioned in the middle of the Cyan lines and upon inspection appears to have fewer points where it predicts the target variable poorly compared to the individual decision trees. In particular, as the gradient of the curve rapidly increases towards the end of the curve the number of points used to calculate the predicted value decreases sharply and Bagging reduces the variance in such predictions and helps combat overfitting (although Bagging is not necessarily in general immune to overfitting!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cgvae)",
   "language": "python",
   "name": "cgvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
