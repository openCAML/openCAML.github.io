{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data for Regression\n",
    "\n",
    "In the tree of depth 1 example that we looked at, the model we fit had two terminal nodes and a single feature. He're we're going to fit a model with eight terminal nodes and two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000 #Number of observations in the training set\n",
    "\n",
    "theta = [4, 4, 10, 12]\n",
    "c = [15, 40, 5, 30, 10]\n",
    "\n",
    "x0 = np.random.uniform(0, 16, n)\n",
    "x1 = np.random.uniform(0, 16, n)\n",
    "\n",
    "x = np.array([x0,x1]).reshape((-1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateY(x, splitPoints, theta, sd):\n",
    "    \n",
    "    \n",
    "    if x[0] > theta[0]:\n",
    "        if x[1] > theta[2]:\n",
    "            y = np.random.normal(c[0], sd, 1)\n",
    "        else:\n",
    "            if x[0] <= theta[3]:\n",
    "                y = np.random.normal(c[1], sd, 1) \n",
    "            else:\n",
    "                y = np.random.normal(c[2], sd, 1)\n",
    "    else: \n",
    "        if x[1] <= theta[1]:\n",
    "            y = np.random.normal(c[3], sd, 1)\n",
    "        else: \n",
    "            y = np.random.normal(c[4], sd, 1)\n",
    "        \n",
    "    return y[0]\n",
    "\n",
    "y = [generateY(i, c, theta, 3) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate features and labels to create dataframe\n",
    "dataFeatures = pd.DataFrame(x)\n",
    "dataFeatures.columns = [f'X{i}' for i in range(2)]\n",
    "\n",
    "dataTarget = pd.DataFrame(y)\n",
    "dataTarget.columns = ['Y']\n",
    "\n",
    "data = pd.concat([dataFeatures, dataTarget], axis = 1)\n",
    "\n",
    "\n",
    "#Split up data into a training and testing set\n",
    "trainTestRatio = 0.05\n",
    "trainData, testData = train_test_split(data, test_size=1-trainTestRatio)\n",
    "\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickly plot the data so we know what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( data['X0'], data['X1'] , c=data['Y'], \n",
    "            cmap = 'inferno', alpha =0.5)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Target value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between the features (the x and y axis in this case) and the target (the corresponding colour of each datapoint) is fairly non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the dataset it's hopefully quite clear that a standard linear regression (without feature engineering) would struggle to model this dataset with a high degree of accuracy, and that a single split point (i.e. the previous decision tree example) won't be enough either. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model this dataset, we're going to require a decision tree with depth > 1\n",
    "\n",
    "(**Disclaimer**: as far as I'm aware this dataset is not representative of any real-world data generation process, I constructed it specifically to show an example where Decision Trees would perform well!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution is going to look slightly different here than it has for the earlier methods we might have seen, where the whole process (fitting, prediction etc.) was contained entirely within a class. \n",
    "\n",
    "We're still going to define a class, but this time our class will be a representation of a node in a tree. Two of the attributes of the node class will be node.left and node.right - node.left and node.right will ALSO be instances of a node (and they will in turn have their own attributes, node.left.left and node.right.left etc.). This structure allows us to naturally capture the hierarchical nature between nodes in a decision tree.\n",
    "\n",
    "Once we have this structure in place, we'll assign further attributes to each node. One of them will be a dataset - the top-level node (the root) will be assigned the whole training set, but as we train our decision tree by obtaining split points to partition the feature space, lower level nodes will be assigned a subset of the training set corresponding to the split points which take us to that node.\n",
    "\n",
    "For a couple of nice diagrams which will allow you to visualise a Decision-Tree structure, check out Bishop's Pattern Recognition and Machine Learning (which you can view here: http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) on pages 663-664\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpler example\n",
    "\n",
    "We can think of the training process as a complicated way of splitting the training set up. Using the method described above we want to split up the dataset into a series of clusters so that we can accurately predict the value of each member of a given cluster.\n",
    "\n",
    "Let's look at a simpler example: Instead of thinking about partitioning a dataset in an optimal fashion, lets just think about how we can use the recursive class structure defined above to split up a dataset. We'll take a sorted array, and we want to define a tree where the terminal nodes contain a single element of the array, with the condition that if a node has two children, we send the lower half of the array associated with the node to the left child and the upper half to the right child\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node: \n",
    "    def __init__(self,arr): \n",
    "        self.left = None #This will be filled by another node, the left child of this node\n",
    "        self.right = None #Right child of this node\n",
    "        self.arr = arr #Array associated with this particular node in the tree\n",
    "        \n",
    "root = Node(np.array(list(range(8)))) #Define the root and assign to it the whole array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We've defined the root of our tree by defining an instance of a Node. The root comes equipped with an array.\n",
    "#We want to define root.left to be a node with the bottom half of root.arr and root.arr to be a node with the\n",
    "#top half of root.arr\n",
    "\n",
    "#Like this:\n",
    "\n",
    "root.left = Node(root.arr[:int(np.floor(root.arr.shape[0]/2))]) #root.left is now also a node\n",
    "root.right = Node(root.arr[int(np.floor(root.arr.shape[0]/2)):]) #root.right is now a node\n",
    "\n",
    "print('Array associated with the root')\n",
    "print(root.arr)\n",
    "\n",
    "print('Arrays associated with the children of the root')\n",
    "print('Left: ', root.left.arr, 'Right: ', root.right.arr)\n",
    "\n",
    "print(root.left, root.right) #We see that these are also instances of a Node, too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then follow the same process on root.left so that it also has two children, root.left.left and root.left.right, each of which is associated with its own smaller array. \n",
    "\n",
    "To save us writing out the above cell a bunch of times we can use a recursive function which keeps on splitting an array until the splits only have one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treeSplitting(Tree):\n",
    "    if len(Tree.arr) > 1: #If the array has length > 1 we need to keep splitting\n",
    "        Tree.left = Node(Tree.arr[:int(np.floor(Tree.arr.shape[0]/2))]) #Split the array and assign it to a child node\n",
    "        Tree.right = Node(Tree.arr[int(np.floor(Tree.arr.shape[0]/2)):])\n",
    "        treeSplitting(Tree.left) #Use recursion to keep on splitting until we reach a single value\n",
    "        treeSplitting(Tree.right)\n",
    "        \n",
    "root = Node(np.array(list(range(8))))\n",
    "treeSplitting(root)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.right.left.right.arr #Checking one of the terminal nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What was the point of all that? \n",
    "\n",
    "Hopefully it gives us some intuition about a tree structure and how we might go about growing a tree. Whilst in this simple example we split up the data by sending the top half left and the bottom half right, in our decision tree example we'll want to determine an optimal split point for the associated dataset and, based on that split point, send one subset of the dataset left and the other subset right for further splitting. \n",
    "\n",
    "Once we've split a certain number of times then we use those terminal datasets to make the predictions for the new values which filter down to that particular terminal node.\n",
    "\n",
    "The key point is that whilst the method we use to determine how to split the dataset might be a bit more complicated than just splitting it in half, in principle it's exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decisionTreeNode:\n",
    "    \n",
    "    def __init__(self,data, target, features, currentDepth): \n",
    "        self.left = None #Left child node\n",
    "        self.right = None #Left child node\n",
    "        self.currentDepth = currentDepth\n",
    "        self.data = data \n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        \n",
    "        \n",
    "        self.splitPointMesh = {}\n",
    "        \n",
    "        for feature in self.features:\n",
    "            #We have a set of features and to determine how we're going to split this dataset \n",
    "            #we'll go through each feature in turn and find an optimal split point for that feature\n",
    "            #Then we'll split using the feature which gives the smallest error for the dataset\n",
    "            #(This is not necessarily an optimal strategy but the combinatorial space is too big to \n",
    "            #investigate every possible tree)\n",
    "            \n",
    "            #So the first point of that is defining a mesh for each feature\n",
    "            \n",
    "\n",
    "\n",
    "            meshMin = np.min(self.data[feature])\n",
    "            meshMax = np.max(self.data[feature])\n",
    "            self.splitPointMesh[feature] = np.linspace(meshMin, meshMax, 500)\n",
    "    \n",
    "    def computeMeansGivenSplitPoint(self, splitPoint, feature):\n",
    "        #Given a split point, we want to split the training set in two\n",
    "        #One containing all the points below the split point and one containing all the points above the split point\n",
    "        #The means are then the means of the targets in those datasets and they are the values we want to return\n",
    "        \n",
    "        #Hint: When using pandas datasets, you can use the .loc command to easily extract a subset of the dataframe\n",
    "        \n",
    "        #Remember we want to return two values\n",
    "    \n",
    "    def computeSquaredError(self, splitPoint, feature):\n",
    "        #Once we have a split point and a set of means, we need to have some way of identifying whether it's \n",
    "        #a good split point or not\n",
    "        \n",
    "        #First apply compuuteMeansGivenSplitPoint to get the values for above and below the dataset\n",
    "        #Then compute the sum of squared errors yielded by assigning the corresponding mean to each point in the training set\n",
    "        #If we add these two sums of squares together then we have a single error number which indicates how good our split point is\n",
    "        \n",
    "        \n",
    "        #Code goes here...\n",
    "        \n",
    "        \n",
    "        #To get the value of errorBelow, subset the training set to the points below the split points\n",
    "        #Then calculate the squared difference between target and c0 for each observation in the subset\n",
    "        #Then sum them up (This can all be done in one line)\n",
    "        \n",
    "        \n",
    "        #Code goes here...\n",
    "        \n",
    "        \n",
    "        totalError = errorBelow + errorAbove\n",
    "\n",
    "        return totalError\n",
    "    \n",
    "    def createSplitDatasetsGivenSplitPointAndFeature(self, splitPoint, feature):\n",
    "        #Given a split point, split the dataset up and return two datasets\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the decisionTreeNode has all the ingredients we need to fit the model, but we need a function to actually fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitDT(Node, maxDepth):\n",
    "    \n",
    "    \n",
    "    if Node.currentDepth < maxDepth:\n",
    "        #if node depth > max depth then we continue to split\n",
    "        \n",
    "        #Do splitting here\n",
    "        #We want to find the best error for each of the features, then use that feature to do the splitting\n",
    "        \n",
    "        errors = {}\n",
    "        for feature in Node.features:\n",
    "            errors[feature] = #use Node.computeSquaredError for each splitPoint in that feature's mesh - save as list\n",
    "            \n",
    "        #Now we want to extract the feature and splitPoint which gave the best overall error\n",
    "        currentBestError = min(errors[Node.features[0]]) + 1 #Initialise\n",
    "        \n",
    "        for feature in Node.features:\n",
    "            if min(errors[feature]) < currentBestError:\n",
    "                bestFeature = feature\n",
    "                currentBestError = min(errors[feature])\n",
    "                bestSplitPoint = Node.splitPointMesh[feature][np.argmin(errors[feature])]\n",
    "                \n",
    "                \n",
    "        #Now we have the best feature to split on and the place where we should split it\n",
    "        #Use Node.createSplitDatasetsGivenSplitPointAndFeature    \n",
    "        \n",
    "        #Record the splitting process\n",
    "        Node.featureSplitOn = bestFeature\n",
    "        Node.bestSplitPoint = bestSplitPoint\n",
    "        #print(bestFeature, bestSplitPoint)\n",
    "        \n",
    "        if Node.data.drop_duplicates().shape[0] > 1:\n",
    "            #i.e. the data is non-empty\n",
    "            #We want the left and right attributes to be decision tree nodes too \n",
    "            Node.left = decisionTreeNode#(...) #Define nodes on the levels below (increment depth by 1)\n",
    "            Node.right = decisionTreeNode#(...)\n",
    "            #Now do the recursive part, which works exactly the same as we saw for the simpler example above\n",
    "\n",
    "        else: #If there is only one example left in this dataset then there's no need to do any splitting\n",
    "            Node.left = copy.deepcopy(Node)\n",
    "            Node.right = copy.deepcopy(Node)\n",
    "            Node.left.currentDepth = Node.currentDepth + 1\n",
    "            Node.right.currentDepth = Node.currentDepth + 1\n",
    "            #Now do the recursive part, which works exactly the same as we saw for the simpler example above\n",
    "\n",
    "        \n",
    "    elif Node.currentDepth == maxDepth:\n",
    "        #If we're at a terminal node then we need to return a value to predict\n",
    "        #Don't need to do any splitting or anything like that, just want to return the mean value\n",
    "        \n",
    "        Node.prediction = #The mean of the targets\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSingleExample(decisionTreeNode, xrow, maxDepth):\n",
    "    #decisionTreeNode should be the root node of a fitted decision tree\n",
    "    #maxDepth needs to be the same maxDepth as the fitted decision tree\n",
    "    \n",
    "    #xrow needs to be a row of a pandas dataframe with the same column names as the features \n",
    "    #in the training set\n",
    "    \n",
    "    if decisionTreeNode.currentDepth < maxDepth:\n",
    "\n",
    "        if xrow[decisionTreeNode.featureSplitOn] < decisionTreeNode.bestSplitPoint:\n",
    "            #Use recursion to send to the left child\n",
    "            return #...\n",
    "        else:\n",
    "            #Use recursion to send to the right child\n",
    "            return #...\n",
    "     \n",
    "\n",
    "    elif decisionTreeNode.currentDepth == maxDepth:\n",
    "        #Now we're at a terminal node, which will have a prediction attribute\n",
    "        return #..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = decisionTreeNode(trainData, 'Y', ['X0', 'X1'], 0)\n",
    "fitDT(root, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, as always we want to see how our predictions reflect 'reality'\n",
    "\n",
    "We could do this in lots of different ways but one way of nicely illustrating how we've done in this case is to predict the value assigned to every value in the whole training set and see how the heat map compares to the true heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['YPred'] = [predictSingleExample(root, row, 3) for index, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( data['X0'], data['X1'] , c=data['Y'], \n",
    "            cmap = 'inferno', alpha =0.5)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Color Intensity')\n",
    "plt.title('True Y Values')\n",
    "plt.xlabel('X0')\n",
    "plt.ylabel('X1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( data['X0'], data['X1'] , c=data['YPred'], \n",
    "            cmap = 'inferno', alpha =0.5)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Color Intensity')\n",
    "plt.title('Predicted Y Values')\n",
    "plt.xlabel('X0')\n",
    "plt.ylabel('X1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So how have we done then?\n",
    "\n",
    "Hopefully we can see that our model has successfully captured at least some of the non-linear structure in the training set (slight differences in colour may reflect different scales on the colour bars). On my implementation the model performed slightly poorly on the values where X0 < 4, partitioning the space with an incorrect value of X1 - that is the price we must be prepared to pay for using a greedy implementation of the algorithm. However hopefully this notebook demonstrates that Decision Trees can be powerful tools for learning non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "An immediate next step would be to enable the model to be pruned after fitting. Pruning means reducing the depth of the tree in some directions and we'd like to prune the nodes which have little or no effect on performance - for example if two sibling terminal nodes tell us to predict an almost identical value for each, then we could simply convert their parent into a terminal node and assign every observation in the amalgamated space the same value.\n",
    "\n",
    "In the real world, Decision Trees are most commonly used a building block for more complex models. Random Forests and Boosting are two examples of widely used algorithms which leverage Decision Trees to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cgvae)",
   "language": "python",
   "name": "cgvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
