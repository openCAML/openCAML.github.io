{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've completed our Bagging notebooks, you've seen that an ensemble of decision trees can make more accurate predictions than a single decision tree due to reduced variance. If you were hyper-critical of bagging you might ask that if we use the same features for each model and there is a substantial degree of overlap in the training sets for each model as a result of the bootstrapping process, how different are these models? Is each model more or less telling us the same thing?\n",
    "\n",
    "One alteration we could make to the modelling process to get a more diverse ensemble is to randomly select a different subset of features which we can use to fit each model. For example, if we had ten features at our disposal, for the first tree in the ensemble we could select, say, 3 of the features at random and use them to model the first bootstrapped dataset. For the second tree we would select 3 different features at random to model the second bootstrapped dataset, and so on. \n",
    "\n",
    "This approach, known as Random Forests, is different from Bagging in two ways:\n",
    "\n",
    "1. By using a different feature set for each model, our ensemble is made up of a more diverse set of models, which will hopefully reduce the variance in the bias-variance decompoition more effectively. This increased diversity also reduces our chances of overfitting to the training set.\n",
    "2. Again by using a different feature set for each model, we can now quantify how important different features are to the fitting process. For example, if we predict values with high accuracy whenever a certain feature is included in a model, and all of the other models in our ensemble predict poorly, then we could conclude that that feature is essential to accurately modelling the target variable.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Decision Trees\n",
    "\n",
    "The main challenge in understanding Random Forests (and Bagging) is in understanding the underlying Decision Trees which they use to make predictions. If you've not already done so, **I would strongly recommend heading over to our notebooks on Decision Trees and becoming familiar with how they work.** The main purpose of this notebook is to help you understand how to create an ensemble of models and provide them with different features, therefore we provide a complete implementation of a Decision Tree for you to use here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap with the Bagging notebook\n",
    "\n",
    "Since Bagging is essentially a special case of Random Forests (where the number of randomly chosen features is equal to the total number of available features), this notebook is very similar to the bagging notebook - in fact there is only one additional line in the Random Forests notebook, that which chooses a random subset of the parameters. \n",
    "\n",
    "I personally think it makes most sense to become familiar with Decision Trees, then extend that idea to Bagging to see how an ensemble of models affects performance, because we can do that when using a single feature (unlike with Random Forests), making visualisation easier, which is why I made a Bagging notebook. I also understand that most people are probably more interested from a practical standpoint in using Random Forests, given their wider use in the real world, which is how we've ended up with two such similar notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data for Random Forest\n",
    "\n",
    "(We'll use a different data generation process to bagging as we want a larger feature set. This is the same data generation process that we used in the Linear Regression Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000 #Number of observations in the training set\n",
    "p = 10 #Number of parameters, including intercept\n",
    "\n",
    "#Assign True parameters to be estimated\n",
    "beta = np.random.uniform(-10, 10, p) #Randomly initialise true parameters\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0,10,(n,(p-1))) \n",
    "X0 = np.array([1]*n).reshape((n,1)) #Columns for intercept\n",
    "\n",
    "X = np.concatenate([X0,X], axis = 1) #Join intercept to other variables to form feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.matmul(X,beta) + np.random.normal(0,10,n) #Linear combination of the features plus a normal error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate to create dataframe\n",
    "\n",
    "dataFeatures = pd.DataFrame(X)\n",
    "dataFeatures.columns = [f'X{i}' for i in range(p)]\n",
    "\n",
    "dataTarget = pd.DataFrame(Y)\n",
    "dataTarget.columns = ['Y']\n",
    "\n",
    "data = pd.concat([dataFeatures, dataTarget], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the feature $X_0$ is always equal to one - this was the intercept term in our linear regression set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Random Forests model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above the random forests implementation is almost identical to the bagging implementation, the only significant difference being an added line in the fitRF (called fitBaggedDT in the bagged implementation) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decisionTreeNode:\n",
    "    \n",
    "    def __init__(self,data, target, features, currentDepth): \n",
    "        self.left = None #Left child node\n",
    "        self.right = None #Left child node\n",
    "        self.currentDepth = currentDepth\n",
    "        self.data = data \n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        \n",
    "        \n",
    "        self.splitPointMesh = {}\n",
    "        \n",
    "        for feature in self.features:\n",
    "            #We have a set of features and to determine how we're going to split this dataset \n",
    "            #we'll go through each feature in turn and find an optimal split point for that feature\n",
    "            #Then we'll split using the feature which gives the smallest error for the dataset\n",
    "            #(This is not necessarily an optimal strategy but the combinatorial space is too big to \n",
    "            #investigate every possible tree)\n",
    "            \n",
    "            #So the first point of that is defining a mesh for each feature\n",
    "            \n",
    "#             meshMin = np.sort(self.data[feature])[1] \n",
    "#             meshMax = np.sort(self.data[feature])[-2]\n",
    "\n",
    "            meshMin = np.min(self.data[feature])\n",
    "            meshMax = np.max(self.data[feature])\n",
    "            self.splitPointMesh[feature] = np.linspace(meshMin, meshMax, 500)\n",
    "    \n",
    "    def computeMeansGivenSplitPoint(self, splitPoint, feature):\n",
    "        #Given a split point, we want to split the training set in two\n",
    "        #One containing all the points below the split point and one containing all the points above the split point\n",
    "        #The means are then the means of the targets in those datasets and they are the values we want to return\n",
    "        \n",
    "        belowSplitPoint = self.data.loc[self.data[feature] < splitPoint][self.target].mean()\n",
    "        aboveSplitPoint = self.data.loc[self.data[feature] >= splitPoint][self.target].mean()\n",
    "        \n",
    "        return belowSplitPoint, aboveSplitPoint\n",
    "    \n",
    "    def computeSquaredError(self, splitPoint, feature):\n",
    "        #Once we have a split point and a set of means, we need to have some way of identifying whether it's \n",
    "        #a good split point or not\n",
    "        \n",
    "        #First apply compuuteMeansGivenSplitPoint to get the values for above and below the dataset\n",
    "        #Then compute the sum of squared errors yielded by assigning the corresponding mean to each point in the training set\n",
    "        #If we add these two sums of squares together then we have a single error number which indicates how good our split point is\n",
    "        \n",
    "        c0, c1 = self.computeMeansGivenSplitPoint(splitPoint, feature)\n",
    "        \n",
    "        #To get the value of errorBelow, subset the training set to the points below the split points\n",
    "        #Then calculate the squared difference between target and c0 for each observation in the subset\n",
    "        #Then sum them up (This can all be done in one line)\n",
    "        \n",
    "        errorBelow = np.sum((self.data.loc[self.data[feature] < splitPoint][self.target] - c0)**2) \n",
    "        \n",
    "        #errorAbove works in the same way\n",
    "        errorAbove = np.sum((self.data.loc[self.data[feature] >= splitPoint][self.target] - c1)**2) \n",
    "        \n",
    "        totalError = errorBelow + errorAbove\n",
    "\n",
    "        return totalError\n",
    "    \n",
    "    def createSplitDatasetsGivenSplitPointAndFeature(self, splitPoint, feature):\n",
    "        #Given a split point, split the dataset up and return two datasets\n",
    "        \n",
    "        belowData = self.data.loc[self.data[feature] < splitPoint]\n",
    "        aboveData = self.data.loc[self.data[feature] >= splitPoint]\n",
    "        \n",
    "        return belowData, aboveData\n",
    "    \n",
    "\n",
    "def fitDT(Node, maxDepth):\n",
    "    \n",
    "    \n",
    "    if Node.currentDepth < maxDepth:\n",
    "        #if node depth > max depth then we continue to split\n",
    "        \n",
    "        #Do splitting here\n",
    "        #We want to find the best error for each of the features, then use that feature to do the splitting\n",
    "        \n",
    "        errors = {}\n",
    "        for feature in Node.features:\n",
    "            errors[feature] = [Node.computeSquaredError(splitPoint, feature) for splitPoint in Node.splitPointMesh[feature]]\n",
    "            \n",
    "        #Now we want to extract the feature and splitPoint which gave the best overall error\n",
    "        currentBestError = min(errors[Node.features[0]]) + 1 #Initialise\n",
    "        \n",
    "        for feature in Node.features:\n",
    "            if min(errors[feature]) < currentBestError:\n",
    "                bestFeature = feature\n",
    "                currentBestError = min(errors[feature])\n",
    "                bestSplitPoint = Node.splitPointMesh[feature][np.argmin(errors[feature])]\n",
    "                \n",
    "                \n",
    "        #Now we have the best feature to split on and the place where we should split it\n",
    "        splitDataLeft, splitDataRight = Node.createSplitDatasetsGivenSplitPointAndFeature(bestSplitPoint, bestFeature)\n",
    "        \n",
    "        #Record the splitting process\n",
    "        Node.featureSplitOn = bestFeature\n",
    "        Node.bestSplitPoint = bestSplitPoint\n",
    "        #print(bestFeature, bestSplitPoint)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if Node.data.drop_duplicates().shape[0] > 1:\n",
    "            Node.left = decisionTreeNode(splitDataLeft, Node.target, Node.features, Node.currentDepth + 1) #Define nodes on the levels below (increment depth by 1)\n",
    "            Node.right = decisionTreeNode(splitDataRight, Node.target, Node.features, Node.currentDepth + 1)\n",
    "            fitDT(Node.left, maxDepth) #The recursive part, which works exactly the same as we saw for the simpler example above\n",
    "            fitDT(Node.right, maxDepth)\n",
    "        else: #If there is only one example left in this dataset then there's no need to do any splitting\n",
    "            Node.left = copy.deepcopy(Node) #Note deepcopy of the node instance\n",
    "            Node.right = copy.deepcopy(Node)\n",
    "            Node.left.currentDepth = Node.currentDepth + 1\n",
    "            Node.right.currentDepth = Node.currentDepth + 1\n",
    "            fitDT(Node.left, maxDepth) #The recursive part, which works exactly the same as we saw for the simpler example above\n",
    "            fitDT(Node.right, maxDepth)\n",
    "            \n",
    "        \n",
    "    elif Node.currentDepth == maxDepth:\n",
    "        #If we're at a terminal node then we need to return a value to predict\n",
    "        #Don't need to do any splitting or anything like that, just want to return the mean value\n",
    "        \n",
    "        Node.prediction = Node.data[Node.target].mean()\n",
    "            \n",
    "def predictSingleExample(decisionTreeNode, xrow, maxDepth):\n",
    "    #decisionTreeNode should be the root node of a fitted decision tree\n",
    "    #maxDepth needs to be the same maxDepth as the fitted decision tree\n",
    "    \n",
    "    #x needs to be a pandas dataframe (with one or more rows) with the same column names as the features \n",
    "    #in the training set\n",
    "    \n",
    "    if decisionTreeNode.currentDepth < maxDepth:\n",
    "\n",
    "        if xrow[decisionTreeNode.featureSplitOn] < decisionTreeNode.bestSplitPoint:\n",
    "            return predictSingleExample(decisionTreeNode.left, xrow, maxDepth)\n",
    "        else:\n",
    "            return predictSingleExample(decisionTreeNode.right, xrow, maxDepth)\n",
    "     \n",
    "\n",
    "    elif decisionTreeNode.currentDepth == maxDepth:\n",
    "        return decisionTreeNode.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class randomForest:\n",
    "    \n",
    "    def __init__(self, data, target, features, trainTestRatio = 0.9, maxDepth = 5):\n",
    "        #data - a pandas dataset \n",
    "        #target - the name of the pandas column which contains the true labels\n",
    "        #feature - the name of the pandas column which we will use to do the regression\n",
    "        #trainTestRatio - the proportion of the entire dataset which we'll use for training\n",
    "                    #   - the rest will be used for testing\n",
    "        \n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        \n",
    "        #Max Depth of decision tree\n",
    "        self.maxDepth = maxDepth\n",
    "        \n",
    "        #Split up data into a training and testing set\n",
    "        self.train, self.test = train_test_split(data, test_size=1-trainTestRatio) #We're doing the train/test split here\n",
    "                                                                                 #Instead of in the DT\n",
    "            \n",
    "    \n",
    "    def fitRF(self, numTrees = 50, numTreeFeatures = None):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #X should be a pandas dataframe\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Random Forest with 25 trees\n",
    "\n",
    "It might take some time for the model to fit, as our implementation is less efficient than the ones in Python machine learning libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = randomForest(data, 'Y', [f'X{i}' for i in range(len(list(data.columns)) - 1)], maxDepth = 3)\n",
    "rf.fitRF(numTrees = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how we did by plotting our predicted values against the true values in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPred = rf.predict(rf.test[[f'X{i}' for i in range(len(list(data.columns)) - 1)]]) #Obtain predictions on test set\n",
    "plt.scatter(rf.test['Y'], testPred)\n",
    "\n",
    "minLim = min(min(rf.test['Y']), min(testPred))\n",
    "maxLim = max(max(rf.test['Y']), max(testPred))\n",
    "\n",
    "\n",
    "plt.plot(np.arange(minLim, maxLim, 1), np.arange(minLim, maxLim, 1), color = 'gold', linewidth = 4) #Line y = x for comparison\n",
    "\n",
    "#Set axes \n",
    "plt.xlim((minLim,maxLim))\n",
    "plt.ylim((minLim,maxLim))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whilst the model hasn't performed disastrously, there is definitely a tendency to underpredict the larger values and overpredict the smaller ones\n",
    "\n",
    "We could say that the model is relying too much on the overall average of the data and not effectively discriminating between different regions of the feature space.\n",
    "\n",
    "With Random Forests, this issue can often be solved by tweaking the model hyperparameters, i.e. adding more trees to our ensemble and increasing the max depth of the constituent decision trees. Given that our implementation is quite a bit slower than the sklearn implementation we're going to use their implementation to see if this is indeed the case - in principle you should be able to obtain the same results using our implementation (and if you want to do that I admire your patience!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the number of trees in our forest\n",
    "\n",
    "First let's verify that the sklearn implementation of the Random Forest gives comparable results to our own using the parameters that we used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "skrf = RandomForestRegressor(n_estimators=25, max_depth=3, max_features='sqrt') #This is the same architecture we were using\n",
    "skrf.fit(rf.train[rf.features], rf.train[rf.target]) #fit the model using out training data\n",
    "\n",
    "minLim = min(min(rf.test['Y']), min(skrf.predict(rf.test[rf.features])))\n",
    "maxLim = max(max(rf.test['Y']), max(skrf.predict(rf.test[rf.features])))\n",
    "\n",
    "\n",
    "plt.scatter(rf.test['Y'], skrf.predict(rf.test[rf.features]))\n",
    "plt.plot(np.arange(minLim, maxLim, 1), np.arange(minLim, maxLim, 1), color = 'gold', linewidth = 4) #Line y = x for comparison\n",
    "\n",
    "#Set axes \n",
    "plt.xlim((minLim,maxLim))\n",
    "plt.ylim((minLim,maxLim))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our own implementation with 25 trees and a maximum tree depth of 3, we tend to significantly overestimate the smaller labels and underestimate the larger ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add more trees, and allow them to be deeper.\n",
    "skrfBigger = RandomForestRegressor(n_estimators=2000, max_depth=9, max_features='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skrfBigger.fit(rf.train[rf.features], rf.train[rf.target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rf.test['Y'], skrfBigger.predict(rf.test[rf.features]))\n",
    "\n",
    "minLim = min(min(rf.test['Y']), min(skrfBigger.predict(rf.test[rf.features])))\n",
    "maxLim = max(max(rf.test['Y']), max(skrfBigger.predict(rf.test[rf.features])))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.arange(minLim, maxLim, 1), np.arange(minLim, maxLim, 1), color = 'gold', linewidth = 4) #Line y = x for comparison\n",
    "\n",
    "#Set axes\n",
    "plt.xlim((minLim,maxLim))\n",
    "plt.ylim((minLim,maxLim))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this case we can see that increasing the complexity of the model allows us to achieve a good fit\n",
    "\n",
    "The line y=x runs through the centre of the point cloud indicating that, most of the time, the model is accurately predicting the label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cgvae)",
   "language": "python",
   "name": "cgvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
