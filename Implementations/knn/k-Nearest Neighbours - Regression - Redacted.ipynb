{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the fundamental concepts of regression/classification models is that, for the most part, observations with similar input will be assigned similar output. The k-Nearest Neighbours algorithm takes this idea to the extreme by assigning a new observation (in the regression case) to be the average of the k observations closest to it in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptually k-NN works exactly the same for a single feature dataset as it does for a higher-dimensional one so we'll use a single feature dataset for easier visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200 #Number of observations\n",
    "X = np.random.uniform(0,5,n)\n",
    "Y = np.array([x*np.sin(x**2) for x in X]) + np.random.normal(0,1,n)\n",
    "\n",
    "data = pd.DataFrame({'X':X, 'Y':Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.scatter(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN:\n",
    "    \n",
    "    def __init__(self, data, target, features, trainTestRatio = 0.9):\n",
    "        \n",
    "        self.target = target\n",
    "        self.features = features \n",
    "        \n",
    "        #Split up data into a training and testing set\n",
    "        self.train, self.test = train_test_split(data, test_size=1-trainTestRatio)\n",
    "       \n",
    "    #There's no fitting process for k-NN, per se. To make predictions we simply examine the training set\n",
    "    #and find the closest examples we can\n",
    "    \n",
    "    def predictSingleExample(self, x, k = 5):\n",
    "        #For a given example x, we find the k examples in the training set where the features are closest\n",
    "        #in terms of euclidean distance\n",
    "        \n",
    "        trainingData = self.train.copy()\n",
    "        \n",
    "        #Compute distance from x to each element of training set\n",
    "        \n",
    "        #sort by distance and then take the first k\n",
    "        \n",
    "        #Now return the mean of those closest k\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X = None, k = 5):\n",
    "        #Predict the values for a dataframe of values (in the same format as the training set)\n",
    "        #Returns a list of the predicted values\n",
    "        \n",
    "        #if no X provided, predict values of the test set\n",
    "        if X is None:\n",
    "            X = self.test\n",
    "        \n",
    "        return [self.predictSingleExample(x, k) for idx, x in X.iterrows()]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mykNN = kNN(data, 'Y', ['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mykNN.test['Pred'] = mykNN.predict(k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mykNN.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15,6))\n",
    "ax = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "\n",
    "ax.scatter(mykNN.test['X'], mykNN.test['Y'] - mykNN.test['Pred'])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y - Pred')\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([-5,5])\n",
    "\n",
    "ax2.scatter(mykNN.test['Y'], mykNN.test['Pred'], label = 'True values vs Predicted Values')\n",
    "ax2.plot(np.arange(-5,5,0.1), np.arange(-5,5,0.1), color = 'green', label = 'Line y = x')\n",
    "ax2.set_xlim([-5, 5])\n",
    "ax2.set_ylim([-5,5])\n",
    "ax2.set_xlabel('True Label')\n",
    "ax2.set_ylabel('Predicted Label')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left plot shows the residuals plotted against out input feature, X. We can see that overall, the residuals are scattered about 0, indicating that in general the model did a decent job of capturing the relationship between the feature and target. The residuals exhibit slighly more variance when $x > 3$, but this is not particularly surpising as the target oscillates for large $x$.\n",
    "\n",
    "If we examine the plot on the right, we can see that the scatter plot of the true vs predicted values generally adheres to the line line $y = x$, indicating the model is performing as we would want.\n",
    "\n",
    "For a conceptually simple algorithm, k-nearest neighbours has worked surprisingly well on this example - far better, for example, than we would expect linear regression to, due to the highly non-linear feature-target relationship.\n",
    "\n",
    "Whilst k-nearest neighbours has performed well on a small dataset such as this one - it tends to scale poorly to larger datasets: Larger datasets demand more memory to store and it takes longer to find the nearest neighbours of a new example. Additionally, k-nearest neighbours suffers from the ['Curse of Dimensionality'](https://en.wikipedia.org/wiki/Curse_of_dimensionality), meaning that its performance tends to degrade substantially as the number of features is increased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the effect of changing k\n",
    "\n",
    "k is a hyperparameter we can tune and should be thought of as a regulariser. The larger the value of k, the more training examples we use in the prediction and therefore the model will be more robust to slight perturbations as we increase k.\n",
    "\n",
    "Let's plot the regression line for a variety of different values of k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "\n",
    "\n",
    "dataRegLine = pd.DataFrame({'X':np.linspace(0,5,100)})\n",
    "\n",
    "for k in [1, 5, 10, 30, 100, 180]:\n",
    "    \n",
    "    dataRegLine[f'Pred{k}'] = mykNN.predict(X = dataRegLine[['X']], k = k) #Obtain predictions\n",
    "    plt.plot(dataRegLine['X'], dataRegLine[f'Pred{k}'], label = f'{k}-Nearest Neighbours') #Plot regression line\n",
    "    \n",
    "plt.legend()\n",
    "\n",
    "plt.scatter(mykNN.train['X'], mykNN.train['Y'])\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the effect of changing the value of k. Having k = 1 yields the most non-linear regression line, but has the disadvantage that it essentially just connect the lines between training points. Conversely setting k = 180 returns the same value for all inputs, as we take the average of the whole training set each time we make a prediction. Intermediate values of k exhibit varying degrees of non-linearity, in line with what we might expect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cgvae)",
   "language": "python",
   "name": "cgvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
