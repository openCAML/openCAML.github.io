{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - A method for binary classification.\n",
    "\n",
    "Despite the fact that we typically use logistic regression for classification problems, it is indeed a regression model. If our target variable is a 0/1 binary variable, the continuous variable we aim to predict is the *probability* that the binary label is equal to 1. \n",
    "\n",
    "\n",
    "Recall that when we use linear regression to predict target variable $y$ conditional on feature vector $x$, we make the modelling assumption that there is a vector of parameters, $\\beta$, such that \n",
    "\n",
    "$$y|x \\sim N(\\beta^Tx, \\sigma^2)$$\n",
    "\n",
    "That is, we assume y follows a normal distribution\\* and try to find its mean.\n",
    "\n",
    "\n",
    "For logistic regression, we make a similar assumption: For target variable $y$ and features $x$ we assume that \n",
    "\n",
    "$$ y|x \\sim \\text{Bin}(1, p(x))$$\n",
    "\n",
    "That is, y follows a binomial distribution where the probability is determined by a function $p(x)$. We want to estimate what the function $p$ is. \n",
    "\n",
    "\\* **Note that we don't assume all of the target variables in a dataset follow the same normal distribution. Rather the target for a single observation has it's own personal distribution which we want to find the mean of.**\n",
    "\n",
    "\n",
    "The method we use to predict this probability bears a certain resemblance to how we fit a linear regression model, but has a couple of extra steps:\n",
    "\n",
    "1. Firstly, when we're doing linear regression, we make a prediction by estimating a parameter vector $\\beta$ and then computing $\\beta^Tx$ for an input vector $x$. This allows us to predict any value between $-\\infty \\text{ and } \\infty$, but if we're trying to predict a probability between 0 and 1, we need to do something different.\n",
    "\n",
    "2. When we're training a linear regression model, we know what the target variable is for all of the variables in the training set during training. Conversely, when training a logistic regression model, we don't know what the probabilty for the target taking a certain value is - we only know the binary value that the target eventually took. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combat the issue of making predictions between 0 and 1, we use a *link function*. We still want to make our predictions using $\\beta^Tx$, but we force the output to be in the right range ($[0,1]$) by passing $\\beta^Tx$ through a function. The general idea is that we need to choose a function such that if $\\beta^Tx$ is large and negative, we want to assign a probability of approximately 0, whereas if it's large and positive we want to assign a probability of approximately 1. A commonly used link function is the function is the function \n",
    "\n",
    "$$f(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "A quick plot of $f(x)$ shows that it exhibits the desired behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-10,10, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = [1/(1 + np.exp(-x)) for x in X]\n",
    "plt.plot(X, fx)\n",
    "plt.show() #We can see that the function is bounded below and above by 0 and 1, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now for the second problem**: For some regression problems you might be familiar with the idea of minimising the sum-of-squared errors of our predictions. That approach won't work here, because we don't know what the true probabilities are so we can't compare our predictions to them.\n",
    "\n",
    "Instead we adopt the principle when training that the more confidently our model predicts, the more it should be penalised if it is subsequently wrong. For example, if our model says that the $P(Y = 1) = 0.95$ and it turns out that $Y = 0$, then it should be penalised more heavily than if it estimates $P(Y = 1) = 0.55$ (in which case it doesn't really know but has to pick one label or the other) when $Y = 0$\n",
    "\n",
    "\n",
    "For a dataset $(y_1, x_1), ..., (y_n, x_n)$, (with $y_i \\in \\{0,1\\}$ and $x_i$ a p-dimensional vector) we want to choose $\\beta$ such that the following function is minimised:\n",
    "\n",
    "$$L(\\beta) = \\sum_{i = 1}^n -(y_i \\times \\log(P(y_i = 1|x_i, \\beta )) + (1 - y_i) \\times \\log(1 - P(y_i = 1|x_i, \\beta )))$$\n",
    "\n",
    "This loss function is known as the *logarithmic* or *cross-entropy loss*.\n",
    "\n",
    "Unlike with linear regression there is no analytical solution to find the minimum of $L(\\beta)$, so we have to adopt an iterative approximation scheme for finding the minimum. The details of the algorithm can be found online (for instance on page 121 of *The Elements of Statistical Learning*), but for the purposes of implementation it proceeds in the following way:\n",
    "\n",
    "1. Randomly initialise $\\beta$ as a random vector of length $p$\n",
    "2. Set $\\beta \\leftarrow \\beta + (X^TWX)^{-1}X^T(y - p)$, where: \n",
    "    * $X$ is the $n \\times p$ dimensional matrix of features\n",
    "    * $y$ is the n-dimensional vector of binary labels\n",
    "    * $p$ is an n-dimensional vector, where $p_i = P(y_i = 1|x_i, \\beta) = \\frac{1}{1 + \\exp(-\\beta^Tx_i)}$ (using the old value of $\\beta$)\n",
    "    * $W$ is a diagonal $n \\times n$ matrix where $W_{i,i} = p_i(1 - p_i)$\n",
    "\n",
    "3. Apply step 2 until $\\beta$ converges\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By looking at the algorithm further, we can understand why it is called *Iteratively Reweighted Least Squares*: \n",
    "\n",
    "**Disclaimer**: This part is a bit algebra heavy so skip ahead to the punchline if you don't care for that sort of thing.\n",
    "\n",
    "\n",
    "From *The Elements of Statistical Learning*, in equation (4.28), we can reformulate step 2 above as:\n",
    "\n",
    "$$\\beta^{new} = argmin_{\\beta}(z - X\\beta)^TW(z - X\\beta)$$\n",
    "\n",
    "Where $z = X\\beta + W^{-1}(y - p)$.\n",
    "\n",
    "By unpacking the definition of $z$, we get that:\n",
    "\n",
    "$$\\beta^{new} = argmin_{\\beta}(X\\beta + W^{-1}(y - p) - X\\beta)^TW(X\\beta + W^{-1}(y - p) - X\\beta)$$\n",
    "\n",
    "We can cancel out the redundant terms, so that: \n",
    "\n",
    "$$\\beta^{new} = argmin_{\\beta}(W^{-1}(y - p))^TW(W^{-1}(y - p))$$ noting that the right hand side of this equation is still dependent on $\\beta$ through $p$.\n",
    "\n",
    "Now by one of the standard properties of matrix transposition \\[$(AB)^T = B^TA^T$\\], we can rearrange the right hand side of the equation to get:\n",
    "\n",
    "$$\\beta^{new} = argmin_{\\beta}((y - p))^TW^{-1}(y - p))$$ where we have cancelled $W$ and $W^{-1}$ and used the fact that $W$ is symmetric (i.e. $(W^{-1})^T = W^{-1}$)\n",
    "\n",
    "### The punchline\n",
    "\n",
    "So (eventually) we can say that under this iterative approach, at each step our new value of our parameter vector $\\beta$ is the one which minimises:\n",
    "\n",
    "$$L(\\beta) = \\sum_{i = 1}^n (y_i - p_i)^2 \\times \\frac{1}{p_i(1 - p_i)}$$ where again we note that $p_i$ is dependent on $\\beta$.\n",
    "\n",
    "\n",
    "This sum is instructive: Similar to linear regression, we're trying to minimise a sum of squared terms. In this case, where we aim to predict the probability an observation had a certain label, the squared term will be small if we correctly predict the label for that example - the higher our degree of confidence the smaller our cost will be (assuming we ended up being correct).\n",
    "\n",
    "In contrast to linear regression, each term in the sum here has an extra product, $\\frac{1}{p_i(1 - p_i)}$. This assigns a weight to each term in the sum of squares. $\\frac{1}{p_i(1 - p_i)}$ is small when $p_i = 0.5$, and large when $p$ is close to 0 or 1 - our penalty is therefore greatly boosted if we predict with a high degree of confidence and get it wrong, which is in line with the heuristic we proposed above. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000 #Number of observations in the training set\n",
    "p = 5 #Number of parameters, including intercept\n",
    "\n",
    "#Assign True parameters to be estimated\n",
    "Beta = np.random.uniform(-10, 10, p) #Randomly initialise true parameters\n",
    "print(Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct feature matrix\n",
    "X = np.random.uniform(0,10,(n,(p-1))) \n",
    "X0 = np.array([1]*n).reshape((n,1)) #Columns for intercept\n",
    "X = np.concatenate([X0,X], axis = 1) #Join intercept to other variables to form feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = np.matmul(X,Beta)  #Linear combination of the features plus a normal error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = 1/(1 + np.exp(-1*eta)) #Compute true (unobservable, in reality) probability that each observation has label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [np.random.binomial(1, prob) for prob in probs] #Sample binary labels\n",
    "pd.Series(Y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate with feature matrix to create dataframe\n",
    "dataFeatures = pd.DataFrame(X)\n",
    "dataFeatures.columns = [f'X{i}' for i in range(p)] \n",
    "\n",
    "dataTarget = pd.DataFrame(Y)\n",
    "dataTarget.columns = ['Y']\n",
    "\n",
    "data = pd.concat([dataFeatures, dataTarget], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() #Quick look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticRegression:\n",
    "    \n",
    "    def __init__(self, data, target, features, trainTestRatio = 0.9):\n",
    "        #data - a pandas dataset \n",
    "        #target - the name of the pandas column which contains the true labels\n",
    "        #features - A list containing the names of the columns which we will use to do the regression\n",
    "        #trainTestRatio - the proportion of the entire dataset which we'll use for training\n",
    "                    #   - the rest will be used for testing\n",
    "        \n",
    "        self.target = target\n",
    "        self.features = features \n",
    "        \n",
    "        #Split up data into a training and testing set\n",
    "        self.train, self.test = train_test_split(data, test_size=1-trainTestRatio)\n",
    "        \n",
    "    #To fit the model, we're going to use the notation adopted in Elements of Statistical Learning\n",
    "    #y will denote the vector of binary labels\n",
    "    #X represents the n x p matrix of features\n",
    "    #p represents the current estimate (using the current parameter vector) of the probabilities for the labels\n",
    "    #W is a n x n diagonal matrix of weights with ith diagonal element p(xi;beta^(old))(1 - p(xi;beta^old))\n",
    "    \n",
    "    \n",
    "    def updateBeta(self, beta, y, X, p, W):\n",
    "        \n",
    "        #Write update step for beta\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def updateP(self, beta, X):\n",
    "        \n",
    "        #update p vector given beta and X \n",
    "        pass\n",
    "    \n",
    "    def updateW(self, beta, X):\n",
    "        \n",
    "        #Update Weight matrix\n",
    "        pass\n",
    "    \n",
    "    def fitLR(self, tolerance = 0.001):\n",
    "        \n",
    "        #Write code to run the iteratively reweighted least squares algorithm until convergence\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def predictProbs(self, X):\n",
    "        #Return the probability of a label being equal to 1\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    def predictLabel(self, X, threshold = 0.5):\n",
    "        #Return the predicted label (1 if P(label = 1) > threshold, otherwise return 0)\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myLR = logisticRegression( data, 'Y', [f'X{i}' for i in range(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myLR.fitLR() #Might take a few seconds to fit - scikit-learn's implementation is evidently much more efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myLR.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the model fit\n",
    "\n",
    "First let's look at our beta vector and see how it compares to the true parameter vector that was used to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, b in enumerate(myLR.beta):\n",
    "    print(round(myLR.beta[idx],3), round(Beta[idx], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if all of the parameter estimates are reasonably close to the true parameters, we can be content that the model is working roughly as it should be\n",
    "\n",
    "### Next let's check the accuracy of the model on the test set by computing the confusion matrix\n",
    "\n",
    "*Note: if you find the model is predicting all of the variables to be the same label, that might be due in part to the random generation of model parameters in the data generation process. Try running the notebook again and check that there is a reasonable number of 0s and 1s in Y*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSetPred = myLR.predictLabel(myLR.test[myLR.features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(myLR.test[myLR.target], testSetPred) #Confusion matrix of true labels against predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, we are able to see that our model is predicting the correct labels most of the time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cgvae)",
   "language": "python",
   "name": "cgvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
